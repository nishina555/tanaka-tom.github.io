{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning  第5回\n",
    "\n",
    "## 今回の目標\n",
    "前回までで実装したニューラルネットワークに学習機能を追加する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ニューラルネットワークの学習\n",
    "\n",
    "### 学習の流れ\n",
    "- サンプルデータを取る\n",
    "- 損失を算出する\n",
    "- 勾配を算出する\n",
    "- 重みを修正\n",
    "- 繰り返す\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### サンプルデータを取る\n",
    "#### ミニバッチ\n",
    "膨大な数のデータを一つずつ処理すると、計算量が増えすぎる。  \n",
    "この問題を解消するため、データ全体の中からランダムで幾つかのデータを取り出して小さな塊(ミニバッチ)を作成し、データの代表として扱う。    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ミニバッチ](images/learning/mini_batch.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失を算出する\n",
    "#### 損失関数\n",
    "ニューラルネットワークが出した結果と実際の正解を比較して、どれだけ精度が悪かったかを出す。  \n",
    "悪さの指標のため、これが0に近づくようにニューラルネットワークに学習を行わせると正答率が上がる。\n",
    "\n",
    "##### 正答率でなく損失を使う理由\n",
    "正答率は変化が段階的。  \n",
    "一方、損失は重みを微小に変更するだけで連続的に値が変化する。  \n",
    "その為以下で説明する勾配法が使える。\n",
    "\n",
    "##### 交差エントロピー誤差\n",
    "\n",
    "$$E=-\\sum _{ k }^{  }{ { t }_{ k }\\log { { y }_{ k } }  } $$\n",
    "\n",
    "${y}_{k}$ はニューラルネットワークの出力  \n",
    "${ t }_{ k }$ は正解ラベル、正解の時だけ 1 で他は 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "元コード：\n",
    "https://github.com/oreilly-japan/deep-learning-from-scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgoAAAFkCAYAAABB1xPiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCpIkqSCDgiRJKsigIEmSCjIoSJKkgv4fStBSzNp2frsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98365bb978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(0.01,1,0.01)\n",
    "y = np.log(x)\n",
    "\n",
    "plt.plot(x,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.51082562376599072"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # 正解は 2\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]  # ニューラルネットワークは 2 を選択(0.6)\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3025850929940455"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0] # ニューラルネットワークは 7 を選択(0.6)\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 勾配を算出する\n",
    "\n",
    "重みの変更によって、損失がどれだけ変わるかが勾配である。  \n",
    "正答率を上げるには勾配が小さくなる方向に重みを設定し直せばよい。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 損失を重みで微分し勾配を出す\n",
    "\n",
    "\n",
    "\n",
    "$$W=\\begin{pmatrix} { w }_{ 11 } & { w }_{ 21 } & { w }_{ 31 } \\\\ { w }_{ 12 } & { w }_{ 22 } & { w }_{ 32 } \\end{pmatrix}$$\n",
    "\n",
    "$$\\frac { \\partial L }{ \\partial W } =\\begin{pmatrix} \\cfrac { \\partial L }{ { \\partial w }_{ 11 } }  & \\cfrac { \\partial L }{ { \\partial w }_{ 21 } }  & \\cfrac { \\partial L }{ { \\partial w }_{ 31 } }  \\\\ \\cfrac { \\partial L }{ { \\partial w }_{ 12 } }  & \\cfrac { \\partial L }{ { \\partial w }_{ 22 } }  & \\cfrac { \\partial L }{ { \\partial w }_{ 32 } }  \\end{pmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _numerical_gradient_no_batch(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 値を元に戻す\n",
    "        \n",
    "    return grad\n",
    "\n",
    "\n",
    "def numerical_gradient(f, X):\n",
    "    if X.ndim == 1:\n",
    "        return _numerical_gradient_no_batch(f, X)\n",
    "    else:\n",
    "        grad = np.zeros_like(X)\n",
    "        \n",
    "        for idx, x in enumerate(X):\n",
    "            grad[idx] = _numerical_gradient_no_batch(f, x)\n",
    "        \n",
    "        return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重みを修正\n",
    "\n",
    "#### 学習率\n",
    "一回の学習でどれだけパラメータを変化させるか。  \n",
    "人間の手で決めてやる必要がある。  \n",
    "大きすぎると値が収束しない。小さすぎると学習が進まない。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 繰り返す\n",
    "以上の流れを繰り返すと損失が0付近に収束し、学習が深まる。はず。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二層ニューラルネットワークの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 0\n",
      "end: 0\n",
      "train acc, test acc | 0.0975166666667, 0.0974\n",
      "start: 1\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)  # 親ディレクトリのファイルをインポートするための設定\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# データの読み込み\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "iters_num = 3001  # 繰り返しの回数を適宜設定する\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1 # 学習率\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "iter_per_epoch = max(train_size / batch_size, 1) # 60000 / 100 = 600 = 1 epoch\n",
    "\n",
    "for i in range(iters_num):\n",
    "    print(\"start: \" + str(i))\n",
    "    \n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 勾配の計算\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # パラメータの更新\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    print(\"end: \" + str(i))\n",
    "    \n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, t_train)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(\"train acc, test acc | \" + str(train_acc) + \", \" + str(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### グラフ描画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(len(train_acc_list))\n",
    "plt.plot(x, train_acc_list, label='train acc')\n",
    "plt.plot(x, test_acc_list, label='test acc', linestyle='--')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
