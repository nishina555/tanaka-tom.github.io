{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLearning ちょっと横道\n",
    "\n",
    "## 目的\n",
    "前回まで作ってきたモデルで MNIST 以外の学習用データでどうなるかを確認する。\n",
    "\n",
    "## タイタニック号乗客の生存予測モデル\n",
    "\n",
    "### Kaggle\n",
    "データ解析のSkillを競うCompetitionサイト\n",
    "\n",
    "### Titanic: Machine Learning from Disaster\n",
    "https://www.kaggle.com/c/titanic/\n",
    "\n",
    "参考サイト: タイタニック号乗客の生存予測モデルを立ててみる - Qiita http://qiita.com/suzumi/items/8ce18bc90c942663d1e6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas\n",
    "行列のデータ整形が得意なライブラリ  \n",
    "anacondaに入ってる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a  b  c  d\n",
      "0  0  0  0  0\n",
      "1  0  0  0  0\n",
      "2  0  0  0  0\n",
      "3  0  0  0  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_sample = pd.read_csv(\"sample.csv\")\n",
    "print(df_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0\n",
      "1    0\n",
      "2    0\n",
      "3    0\n",
      "Name: a, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_sample['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nL0mNM+glqXEGvSQ1zqCXpMYZ9JLUuP8DDFbj/PNy81YAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ffaff233438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"train.csv\").replace(\"male\",0).replace(\"female\",1)\n",
    "\n",
    "split_data = []\n",
    "for survived in [0,1]:\n",
    "    split_data.append(df[df.Survived==survived])\n",
    "\n",
    "temp = [i[\"Pclass\"].dropna() for i in split_data]\n",
    "plt.hist(temp, histtype=\"barstacked\", bins=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "from collections import OrderedDict\n",
    "\n",
    "class Affine:\n",
    "    def __init__(self, W, b):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.x = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W) + self.b\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dx = np.dot(dout, self.W.T)\n",
    "        self.dW = np.dot(self.x.T, dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx\n",
    "    \n",
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None\n",
    "        self.y = None\n",
    "        self.x = None\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.t = t\n",
    "        self.y = softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\n",
    "\n",
    "        return self.loss\n",
    "\n",
    "    def backward(self, dout=1):\n",
    "        batch_size = self.t.shape[0]\n",
    "        dx = (self.y - self.t) / batch_size\n",
    "\n",
    "        return dx\n",
    "\n",
    "class Relu:\n",
    "    def __init__(self):\n",
    "        self.mask = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.mask = (x <= 0)\n",
    "        out = x.copy()\n",
    "        out[self.mask] = 0\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        dout[self.mask] = 0\n",
    "        dx = dout\n",
    "\n",
    "        return dx\n",
    "\n",
    "def softmax(x):\n",
    "    if x.ndim == 2:\n",
    "        x = x.T\n",
    "        x = x - np.max(x, axis=0)\n",
    "        y = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "        return y.T \n",
    "\n",
    "    x = x - np.max(x) # オーバーフロー対策\n",
    "    return np.exp(x) / np.sum(np.exp(x))\n",
    "\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t])) / batch_size\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std = 0.01):\n",
    "        # 重みの初期化\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size) \n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "        # レイヤの生成\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Affine1'] = Affine(self.params['W1'], self.params['b1'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Affine2'] = Affine(self.params['W2'], self.params['b2'])\n",
    "\n",
    "        self.lastLayer = SoftmaxWithLoss()\n",
    "        \n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    # x:入力データ, t:教師データ\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.lastLayer.forward(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        # forward\n",
    "        self.loss(x, t)\n",
    "\n",
    "        # backward\n",
    "        dout = 1\n",
    "        dout = self.lastLayer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "\n",
    "        # 設定\n",
    "        grads = {}\n",
    "        grads['W1'], grads['b1'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\n",
    "\n",
    "        return grads\n",
    "\n",
    "def backprop_learn(x_train, t_train, test_data):\n",
    "    \n",
    "    network = TwoLayerNet(input_size=6, hidden_size=10, output_size=2)\n",
    "\n",
    "    iters_num = 102*100+1\n",
    "    train_size = x_train.shape[0]\n",
    "    batch_size = 102\n",
    "    learning_rate = 0.1\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "    iter_per_epoch = max(train_size / batch_size, 1)\n",
    "    print(iter_per_epoch)\n",
    "    for i in range(iters_num):\n",
    "        batch_mask = np.random.choice(train_size, batch_size)\n",
    "        x_batch = x_train[batch_mask]\n",
    "        t_batch = t_train[batch_mask]\n",
    "\n",
    "        grad = network.gradient(x_batch, t_batch)\n",
    "\n",
    "        for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "            network.params[key] -= learning_rate * grad[key]\n",
    "\n",
    "        loss = network.loss(x_batch, t_batch)\n",
    "        train_loss_list.append(loss)\n",
    "\n",
    "        if i % iter_per_epoch == 0:\n",
    "            \n",
    "            train_acc = network.accuracy(x_train, t_train)\n",
    "            train_acc_list.append(train_acc)\n",
    "            print(i, train_acc)\n",
    "    predict_data = network.predict(test_data)\n",
    "    return np.argmax(predict_data, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "714\n",
      "7.0\n",
      "0 0.413165266106\n",
      "7 0.627450980392\n",
      "14 0.670868347339\n",
      "21 0.682072829132\n",
      "28 0.621848739496\n",
      "35 0.662464985994\n",
      "42 0.669467787115\n",
      "49 0.642857142857\n",
      "56 0.593837535014\n",
      "63 0.670868347339\n",
      "70 0.652661064426\n",
      "77 0.656862745098\n",
      "84 0.634453781513\n",
      "91 0.656862745098\n",
      "98 0.593837535014\n",
      "105 0.665266106443\n",
      "112 0.649859943978\n",
      "119 0.665266106443\n",
      "126 0.593837535014\n",
      "133 0.593837535014\n",
      "140 0.655462184874\n",
      "147 0.593837535014\n",
      "154 0.593837535014\n",
      "161 0.593837535014\n",
      "168 0.593837535014\n",
      "175 0.593837535014\n",
      "182 0.593837535014\n",
      "189 0.593837535014\n",
      "196 0.593837535014\n",
      "203 0.593837535014\n",
      "210 0.593837535014\n",
      "217 0.593837535014\n",
      "224 0.593837535014\n",
      "231 0.593837535014\n",
      "238 0.593837535014\n",
      "245 0.593837535014\n",
      "252 0.593837535014\n",
      "259 0.593837535014\n",
      "266 0.593837535014\n",
      "273 0.593837535014\n",
      "280 0.593837535014\n",
      "287 0.593837535014\n",
      "294 0.593837535014\n",
      "301 0.593837535014\n",
      "308 0.593837535014\n",
      "315 0.593837535014\n",
      "322 0.593837535014\n",
      "329 0.593837535014\n",
      "336 0.593837535014\n",
      "343 0.593837535014\n",
      "350 0.593837535014\n",
      "357 0.593837535014\n",
      "364 0.593837535014\n",
      "371 0.593837535014\n",
      "378 0.593837535014\n",
      "385 0.593837535014\n",
      "392 0.593837535014\n",
      "399 0.593837535014\n",
      "406 0.593837535014\n",
      "413 0.593837535014\n",
      "420 0.593837535014\n",
      "427 0.593837535014\n",
      "434 0.593837535014\n",
      "441 0.593837535014\n",
      "448 0.593837535014\n",
      "455 0.593837535014\n",
      "462 0.593837535014\n",
      "469 0.593837535014\n",
      "476 0.593837535014\n",
      "483 0.593837535014\n",
      "490 0.593837535014\n",
      "497 0.593837535014\n",
      "504 0.593837535014\n",
      "511 0.593837535014\n",
      "518 0.593837535014\n",
      "525 0.593837535014\n",
      "532 0.593837535014\n",
      "539 0.593837535014\n",
      "546 0.593837535014\n",
      "553 0.593837535014\n",
      "560 0.593837535014\n",
      "567 0.593837535014\n",
      "574 0.593837535014\n",
      "581 0.593837535014\n",
      "588 0.593837535014\n",
      "595 0.593837535014\n",
      "602 0.593837535014\n",
      "609 0.593837535014\n",
      "616 0.593837535014\n",
      "623 0.593837535014\n",
      "630 0.593837535014\n",
      "637 0.593837535014\n",
      "644 0.593837535014\n",
      "651 0.593837535014\n",
      "658 0.593837535014\n",
      "665 0.593837535014\n",
      "672 0.593837535014\n",
      "679 0.593837535014\n",
      "686 0.593837535014\n",
      "693 0.593837535014\n",
      "700 0.593837535014\n",
      "707 0.593837535014\n",
      "714 0.593837535014\n",
      "721 0.593837535014\n",
      "728 0.593837535014\n",
      "735 0.593837535014\n",
      "742 0.593837535014\n",
      "749 0.593837535014\n",
      "756 0.593837535014\n",
      "763 0.593837535014\n",
      "770 0.593837535014\n",
      "777 0.593837535014\n",
      "784 0.593837535014\n",
      "791 0.593837535014\n",
      "798 0.593837535014\n",
      "805 0.593837535014\n",
      "812 0.593837535014\n",
      "819 0.593837535014\n",
      "826 0.593837535014\n",
      "833 0.593837535014\n",
      "840 0.593837535014\n",
      "847 0.593837535014\n",
      "854 0.593837535014\n",
      "861 0.593837535014\n",
      "868 0.593837535014\n",
      "875 0.593837535014\n",
      "882 0.593837535014\n",
      "889 0.593837535014\n",
      "896 0.593837535014\n",
      "903 0.593837535014\n",
      "910 0.593837535014\n",
      "917 0.593837535014\n",
      "924 0.593837535014\n",
      "931 0.593837535014\n",
      "938 0.593837535014\n",
      "945 0.593837535014\n",
      "952 0.593837535014\n",
      "959 0.593837535014\n",
      "966 0.593837535014\n",
      "973 0.593837535014\n",
      "980 0.593837535014\n",
      "987 0.593837535014\n",
      "994 0.593837535014\n",
      "1001 0.593837535014\n",
      "1008 0.593837535014\n",
      "1015 0.593837535014\n",
      "1022 0.593837535014\n",
      "1029 0.593837535014\n",
      "1036 0.593837535014\n",
      "1043 0.593837535014\n",
      "1050 0.593837535014\n",
      "1057 0.593837535014\n",
      "1064 0.593837535014\n",
      "1071 0.593837535014\n",
      "1078 0.593837535014\n",
      "1085 0.593837535014\n",
      "1092 0.593837535014\n",
      "1099 0.593837535014\n",
      "1106 0.593837535014\n",
      "1113 0.593837535014\n",
      "1120 0.593837535014\n",
      "1127 0.593837535014\n",
      "1134 0.593837535014\n",
      "1141 0.593837535014\n",
      "1148 0.593837535014\n",
      "1155 0.593837535014\n",
      "1162 0.593837535014\n",
      "1169 0.593837535014\n",
      "1176 0.593837535014\n",
      "1183 0.593837535014\n",
      "1190 0.593837535014\n",
      "1197 0.593837535014\n",
      "1204 0.593837535014\n",
      "1211 0.593837535014\n",
      "1218 0.593837535014\n",
      "1225 0.593837535014\n",
      "1232 0.593837535014\n",
      "1239 0.593837535014\n",
      "1246 0.593837535014\n",
      "1253 0.593837535014\n",
      "1260 0.593837535014\n",
      "1267 0.593837535014\n",
      "1274 0.593837535014\n",
      "1281 0.593837535014\n",
      "1288 0.593837535014\n",
      "1295 0.593837535014\n",
      "1302 0.593837535014\n",
      "1309 0.593837535014\n",
      "1316 0.593837535014\n",
      "1323 0.593837535014\n",
      "1330 0.593837535014\n",
      "1337 0.593837535014\n",
      "1344 0.593837535014\n",
      "1351 0.593837535014\n",
      "1358 0.593837535014\n",
      "1365 0.593837535014\n",
      "1372 0.593837535014\n",
      "1379 0.593837535014\n",
      "1386 0.593837535014\n",
      "1393 0.593837535014\n",
      "1400 0.593837535014\n",
      "1407 0.593837535014\n",
      "1414 0.593837535014\n",
      "1421 0.593837535014\n",
      "1428 0.593837535014\n",
      "1435 0.593837535014\n",
      "1442 0.593837535014\n",
      "1449 0.593837535014\n",
      "1456 0.593837535014\n",
      "1463 0.593837535014\n",
      "1470 0.593837535014\n",
      "1477 0.593837535014\n",
      "1484 0.593837535014\n",
      "1491 0.593837535014\n",
      "1498 0.593837535014\n",
      "1505 0.593837535014\n",
      "1512 0.593837535014\n",
      "1519 0.593837535014\n",
      "1526 0.593837535014\n",
      "1533 0.593837535014\n",
      "1540 0.593837535014\n",
      "1547 0.593837535014\n",
      "1554 0.593837535014\n",
      "1561 0.593837535014\n",
      "1568 0.593837535014\n",
      "1575 0.593837535014\n",
      "1582 0.593837535014\n",
      "1589 0.593837535014\n",
      "1596 0.593837535014\n",
      "1603 0.593837535014\n",
      "1610 0.593837535014\n",
      "1617 0.593837535014\n",
      "1624 0.593837535014\n",
      "1631 0.593837535014\n",
      "1638 0.593837535014\n",
      "1645 0.593837535014\n",
      "1652 0.593837535014\n",
      "1659 0.593837535014\n",
      "1666 0.593837535014\n",
      "1673 0.593837535014\n",
      "1680 0.593837535014\n",
      "1687 0.593837535014\n",
      "1694 0.593837535014\n",
      "1701 0.593837535014\n",
      "1708 0.593837535014\n",
      "1715 0.593837535014\n",
      "1722 0.593837535014\n",
      "1729 0.593837535014\n",
      "1736 0.593837535014\n",
      "1743 0.593837535014\n",
      "1750 0.593837535014\n",
      "1757 0.593837535014\n",
      "1764 0.593837535014\n",
      "1771 0.593837535014\n",
      "1778 0.593837535014\n",
      "1785 0.593837535014\n",
      "1792 0.593837535014\n",
      "1799 0.593837535014\n",
      "1806 0.593837535014\n",
      "1813 0.593837535014\n",
      "1820 0.593837535014\n",
      "1827 0.593837535014\n",
      "1834 0.593837535014\n",
      "1841 0.593837535014\n",
      "1848 0.593837535014\n",
      "1855 0.593837535014\n",
      "1862 0.593837535014\n",
      "1869 0.593837535014\n",
      "1876 0.593837535014\n",
      "1883 0.593837535014\n",
      "1890 0.593837535014\n",
      "1897 0.593837535014\n",
      "1904 0.593837535014\n",
      "1911 0.593837535014\n",
      "1918 0.593837535014\n",
      "1925 0.593837535014\n",
      "1932 0.593837535014\n",
      "1939 0.593837535014\n",
      "1946 0.593837535014\n",
      "1953 0.593837535014\n",
      "1960 0.593837535014\n",
      "1967 0.593837535014\n",
      "1974 0.593837535014\n",
      "1981 0.593837535014\n",
      "1988 0.593837535014\n",
      "1995 0.593837535014\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "df[[\"Survived\"]]=df[[\"Survived\"]].astype(int)\n",
    "df2 = df.drop([\"PassengerId\",\"Name\" , \"Ticket\", \"Cabin\", \"Embarked\"], axis=1).dropna()\n",
    "train_data = df2.values\n",
    "x_train = train_data[:, 1:] # Pclass以降の変数\n",
    "train  = train_data[:, 0]  # 正解データ\n",
    "\n",
    "t_train = np.zeros((train.shape[0], 2))\n",
    "t_train[np.arange(train.shape[0]), train.astype(int)] = 1\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"test.csv\").replace(\"male\",0).replace(\"female\",1)\n",
    "test_df[\"Age\"].fillna(test_df.Age.median(), inplace=True)\n",
    "test_df[\"Fare\"].fillna(test_df.Fare.median(), inplace=True)\n",
    "\n",
    "test_df2 = test_df.drop([\"PassengerId\" ,\"Name\" , \"Ticket\" , \"Cabin\", \"Embarked\"], axis=1)\n",
    "test_data = test_df2.values\n",
    "\n",
    "print(x_train.shape[0])\n",
    "\n",
    "predict_data = backprop_learn(x_train, t_train, test_data)\n",
    "\n",
    "print(predict_data)\n",
    "\n",
    "with open(\"predict_result_data.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f, lineterminator='\\n')\n",
    "    writer.writerow([\"PassengerId\", \"Survived\"])\n",
    "    for pid, survived in zip(test_df[\"PassengerId\"].values.astype(int), predict_data.astype(int)):\n",
    "        writer.writerow([pid, survived])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
